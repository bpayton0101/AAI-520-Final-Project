{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "243cc8a9-f2a2-4bff-9e9e-4ce17a16c2e3",
      "metadata": {
        "id": "243cc8a9-f2a2-4bff-9e9e-4ce17a16c2e3"
      },
      "source": [
        "# Load the data files from the Cornell Movie Dialog Corpus\n",
        "\n",
        " https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0720e294-28b6-4764-8c99-ed208e6b956d",
      "metadata": {
        "id": "0720e294-28b6-4764-8c99-ed208e6b956d",
        "outputId": "1becd40e-9e5b-443b-d9c4-e32be9ea45b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset files loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the dataset files (adjust if needed)\n",
        "data_dir = '../archive'\n",
        "\n",
        "# Files we need from the corpus\n",
        "lines_file = os.path.join(data_dir, 'movie_lines.txt')\n",
        "conversations_file = os.path.join(data_dir, 'movie_conversations.txt')\n",
        "\n",
        "# Check if the files exist\n",
        "if os.path.exists(lines_file) and os.path.exists(conversations_file):\n",
        "    print(\"Dataset files loaded successfully.\")\n",
        "else:\n",
        "    print(\"Dataset files are missing. Please download and provide the correct paths.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f302d6-6b72-4ecd-9a8e-a6ed0562ace5",
      "metadata": {
        "id": "65f302d6-6b72-4ecd-9a8e-a6ed0562ace5"
      },
      "source": [
        "# Read the lines from the movies\n",
        "\n",
        "From README.txt\n",
        "\n",
        "movie_lines.txt\n",
        "\t- contains the actual text of each utterance\n",
        "\t- fields:\n",
        "\t\t- lineID\n",
        "\t\t- characterID (who uttered this phrase)\n",
        "\t\t- movieID\n",
        "\t\t- character name\n",
        "\t\t- text of the utterance\n",
        "\n",
        "Example:\n",
        "L868 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ The \"real you\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d093de-16c1-4ced-b0fd-a34b8d895fd9",
      "metadata": {
        "id": "f7d093de-16c1-4ced-b0fd-a34b8d895fd9",
        "outputId": "69800040-a0e0-49e9-cd30-39f27511e367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 304446 lines from the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Load the movie lines from the movie_lines file and create a dictionary\n",
        "def load_lines(file_path):\n",
        "    # Dictionary of Lines with Line ID and it's corresponding text\n",
        "    lines = {}\n",
        "\n",
        "    # with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            # Movie lines are in this format: LineID + Speaker + MovieID + Character + Text\n",
        "            parts = line.strip().split(\" +++$+++ \")\n",
        "            # Perform an explicit check to ensure that the length of parts is exactly 5 before mapping the movie lines.\n",
        "            # Avoid issues if any lines are malformed.\n",
        "            if len(parts) == 5:\n",
        "                line_id = parts[0]     #Line Id\n",
        "                text = parts[4]        # Movie Line\n",
        "                lines[line_id] = text  #Populate dictionary.\n",
        "    return lines\n",
        "\n",
        "# Load movie lines\n",
        "lines = load_lines(lines_file)\n",
        "print(f\"Loaded {len(lines)} lines from the dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f800c71-7769-4478-95d8-8dd03b0f2f1b",
      "metadata": {
        "id": "5f800c71-7769-4478-95d8-8dd03b0f2f1b"
      },
      "source": [
        "# Read the conversations from the movies\n",
        "\n",
        "    Check to see if these conversations' lines are read within movie_lines.\n",
        "    If so, keep it.\n",
        "  \n",
        "- movie_conversations.txt\n",
        "\t- the structure of the conversations\n",
        "\t- fields\n",
        "\t\t- characterID of the first character involved in the conversation\n",
        "\t\t- characterID of the second character involved in the conversation\n",
        "\t\t- movieID of the movie in which the conversation occurred\n",
        "\t\t- list of the utterances that make the conversation, in chronological\n",
        "\t\t\torder: ['lineID1','lineID2',Ã‰,'lineIDN']\n",
        "\t\t\thas to be matched with movie_lines.txt to reconstruct the actual content\n",
        "\n",
        "Example:\n",
        "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L404', 'L405', 'L406', 'L407']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64f10d0-c860-4182-9552-2bc7d7330ab1",
      "metadata": {
        "id": "b64f10d0-c860-4182-9552-2bc7d7330ab1",
        "outputId": "459f8a29-661a-4198-a9ff-6c5c9c4f1479"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Load the movie conversations\n",
        "def load_conversations(file_path, lines):\n",
        "    conversations = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            # Movie conversations are in this format: Character1 + Character2 + MovieID + List of LineIDs\n",
        "            parts = line.strip().split(\" +++$+++ \")\n",
        "            #retrieves line id's\n",
        "            if len(parts) == 4:\n",
        "                # Extract the list of line IDs and convert to text\n",
        "                line_ids = eval(parts[3])  # eval to convert the string list to a list object\n",
        "                conv = [lines[line_id] for line_id in line_ids if line_id in lines] # if line id matches entry in lines dictionary, keep it\n",
        "                conversations.append(conv)\n",
        "    return conversations\n",
        "\n",
        "# Load conversations\n",
        "conversations = load_conversations(conversations_file, lines)\n",
        "print(f\"Loaded {len(conversations)} conversations.\")\n",
        "\n",
        "print(conversations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c6777d-1c5c-4edc-b194-35f944d9624a",
      "metadata": {
        "id": "25c6777d-1c5c-4edc-b194-35f944d9624a"
      },
      "source": [
        "# Exploratory Data analysis\n",
        "# Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53166818-2db0-4beb-b53b-096bc895a406",
      "metadata": {
        "id": "53166818-2db0-4beb-b53b-096bc895a406"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sentence):\n",
        "    # Remove punctuation\n",
        "    sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
        "    # Remove non-letter characters\n",
        "    sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66cbb860-888f-4719-8a69-917d30d64811",
      "metadata": {
        "id": "66cbb860-888f-4719-8a69-917d30d64811"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import contractions # Removing contractions\n",
        "import emoji # Convert Emoticons to Text if you want to perform sentiment analysis.\n",
        "import unicodedata\n",
        "\n",
        "#nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "# Normalize string by converting unicode characters to ASCII and removing non-letters\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Function to convert emojis to words using emoji library mapping\n",
        "def convert_emojis_to_words(text):\n",
        "    converted_text = emoji.demojize(text)\n",
        "    return converted_text\n",
        "\n",
        "def remove_email(text):\n",
        "    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+_-]+)', \"\", text)\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # Regular expression pattern to match emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "        u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Clitics\n",
        "def clean_clitics(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    #text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def remove_digitsInText(text):\n",
        "    textWithoutDigits = list(filter(lambda x: x.isalpha(), text))\n",
        "    return textWithoutDigits\n",
        "\n",
        "def clean_text(text):\n",
        "    # Step 1: Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    #Step 2: Remove emails\n",
        "    text = remove_email(text)\n",
        "\n",
        "    # Step 3: Remove Hashtags\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Step 4: Remove Usernames (assuming they start with '@')\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Step 5: Convert emojis to words\n",
        "    text = convert_emojis_to_words(text)\n",
        "\n",
        "    #Step 6: Remove any emoticons or emojis\n",
        "    text = remove_emojis(text)\n",
        "\n",
        "    # Step 7: Normalize Unicode characters to ASCII\n",
        "    text = unicode_to_ascii(text.lower().strip())\n",
        "\n",
        "    # Step 8: Remove punctuation, numbers, and extra spaces\n",
        "    # Step 9: Remove any special characters\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters and numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    #text = re.sub(r\"[^\\w\\s]\", '', text) #Remove special characters\n",
        "    #text = remove_digitsInText(text) # Remove digits in text , Remove any words with digits like 5pm\n",
        "\n",
        "\n",
        "    # Step 10: Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 11: Handle contractions / clitics\n",
        "    text = contractions.fix(text)\n",
        "    text = clean_clitics(text)\n",
        "\n",
        "    # Step 12: Remove non-English words\n",
        "    text = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c6284b-bd2b-42aa-ba5e-a9efe554908b",
      "metadata": {
        "id": "73c6284b-bd2b-42aa-ba5e-a9efe554908b",
        "outputId": "9b9c39d0-f4a0-4f44-f1a1-4090e4d1b42c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4, 5, 6, 7]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
        "        self.num_words = 4  # Count SOS, EOS, UNK, PAD\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def sentence_to_indexes(self, sentence):\n",
        "        return [self.word2index.get(word, 3) for word in sentence.split(' ')]  # 3 is UNK\n",
        "\n",
        "# Example usage\n",
        "vocab = Vocabulary()\n",
        "vocab.add_sentence(\"hello how are you\")\n",
        "print(vocab.sentence_to_indexes(\"hello how are you\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa74572d-a6d0-499c-a33d-de026425335a",
      "metadata": {
        "id": "fa74572d-a6d0-499c-a33d-de026425335a"
      },
      "outputs": [],
      "source": [
        "# Load and clean the data\n",
        "def clean_and_prepare_data(lines_file, conversations_file):\n",
        "    # lines = load_lines(lines_file)\n",
        "    # conversations = load_conversations(conversations_file, lines)\n",
        "\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for conversation in conversations:\n",
        "        for i in range(len(conversation) - 1):\n",
        "            questions.append(conversation[i])\n",
        "            answers.append(conversation[i + 1])\n",
        "\n",
        "    clean_questions = [clean_text(q) for q in questions]\n",
        "    clean_answers = [clean_text(a) for a in answers]\n",
        "\n",
        "    clean_questions_to_indexes = []\n",
        "    clean_answers_to_indexes = []\n",
        "\n",
        "    # Filtering out short or long questions/answers\n",
        "    # Keep only those sentences that have between 2 and 25 words.\n",
        "    # Append the <EOS> (End of String) token to the end of each answer, indicating the end of the response for the model.\n",
        "    filtered_questions, filtered_answers = [], []\n",
        "\n",
        "    for q, a in zip(clean_questions, clean_answers):\n",
        "        vocab.add_sentence(q)\n",
        "        vocab.add_sentence(a)\n",
        "        clean_questions_to_indexes.append(vocab.sentence_to_indexes(q))\n",
        "        clean_answers_to_indexes.append(vocab.sentence_to_indexes(a))\n",
        "\n",
        "        if 2 <= len(q.split()) <= 25 and 2 <= len(a.split()) <= 25:\n",
        "            filtered_questions.append(q)\n",
        "            filtered_answers.append(a + ' <EOS>')  # Append end token to answers\n",
        "\n",
        "    return filtered_questions, filtered_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f72d414-78f1-4d4f-ad3a-d6e4d0d2538a",
      "metadata": {
        "id": "0f72d414-78f1-4d4f-ad3a-d6e4d0d2538a",
        "outputId": "b2b286fb-dd28-4a2a-e63d-6d3d6b085e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of cleaned questions: 152943\n",
            "Number of cleaned answers: 152943\n"
          ]
        }
      ],
      "source": [
        "# Load and clean the data\n",
        "clean_questions, clean_answers = clean_and_prepare_data(lines_file, conversations_file)\n",
        "\n",
        "# Print out the number of cleaned questions and answers\n",
        "print(f\"Number of cleaned questions: {len(clean_questions)}\")\n",
        "print(f\"Number of cleaned answers: {len(clean_answers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0cb623-944c-497a-84df-ef046aa6e666",
      "metadata": {
        "id": "4b0cb623-944c-497a-84df-ef046aa6e666",
        "outputId": "d0210814-30ed-4997-8069-37b89951d924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total valid questions: 151703\n",
            "Total valid answers: 151703\n",
            "Total invalid pairs: 1240\n"
          ]
        }
      ],
      "source": [
        "# Validate the questions and answers\n",
        "def validate_questions_answers(questions, answers, min_len=2, max_len=25):\n",
        "    valid_questions = []\n",
        "    valid_answers = []\n",
        "    invalid_count = 0\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        # Check for non-empty strings and length constraints\n",
        "        if isinstance(question, str) and isinstance(answer, str) and min_len <= len(question.split()) <= max_len and min_len <= len(answer.split()) <= max_len:\n",
        "            valid_questions.append(question)\n",
        "            valid_answers.append(answer)\n",
        "        else:\n",
        "            invalid_count += 1  # Count invalid pairs\n",
        "\n",
        "    print(f\"Total valid questions: {len(valid_questions)}\")\n",
        "    print(f\"Total valid answers: {len(valid_answers)}\")\n",
        "    print(f\"Total invalid pairs: {invalid_count}\")\n",
        "\n",
        "    return valid_questions, valid_answers\n",
        "\n",
        "# Run validation\n",
        "validated_questions, validated_answers = validate_questions_answers(clean_questions, clean_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c07b246f-3761-41b8-8beb-0d123b3024d9",
      "metadata": {
        "id": "c07b246f-3761-41b8-8beb-0d123b3024d9"
      },
      "source": [
        "# Train, Test Split before training using Cornell Movie corpus data\n",
        "\n",
        "Train Set: Use this to train the model.\n",
        "\n",
        "Validation Set: Use this to fine tune the models hyperparameter and evaluate the model during training. Monitor overfitting during training.\n",
        "\n",
        "Test Set: Use this post training to test how well the model generalizes or to see how the model performs on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4f3572e-c6fd-4047-8962-dd6bdae15dda",
      "metadata": {
        "id": "b4f3572e-c6fd-4047-8962-dd6bdae15dda",
        "outputId": "49d10220-18e4-469b-ef17-87548b19920b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 111718\n",
            "Validation set size: 12414\n",
            "Test set size: 31033\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # First split into train and test sets (80% train, 20% test)\n",
        "# train_data, test_data = train_test_split(tokenized_conversations, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Further split the train data into train and validation sets (90% train, 10% validation)\n",
        "# train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "\n",
        "# # Output the sizes of each set\n",
        "# print(f\"Training set size: {len(train_data)}\")\n",
        "# print(f\"Validation set size: {len(val_data)}\")\n",
        "# print(f\"Test set size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ed4a6f-f809-44dd-823d-b63df27ba095",
      "metadata": {
        "id": "84ed4a6f-f809-44dd-823d-b63df27ba095",
        "outputId": "0cb41baf-d781-4259-8121-92852b9f53c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 106192\n",
            "Validation set size: 30340\n",
            "Test set size: 15171\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "def split_data(clean_questions, clean_answers, test_size=0.2, val_size=0.1):\n",
        "    # First split into train and remaining (which will be split further)\n",
        "    questions_train, questions_rem, answers_train, answers_rem = train_test_split(\n",
        "        clean_questions, clean_answers, test_size=(test_size + val_size), random_state=42)\n",
        "\n",
        "    # Then split the remaining into validation and test\n",
        "    val_size_adjusted = val_size / (test_size + val_size)\n",
        "    questions_val, questions_test, answers_val, answers_test = train_test_split(\n",
        "        questions_rem, answers_rem, test_size=val_size_adjusted, random_state=42)\n",
        "\n",
        "    return (questions_train, answers_train), (questions_val, answers_val), (questions_test, answers_test)\n",
        "\n",
        "(train_questions, train_answers), (val_questions, val_answers), (test_questions, test_answers) = split_data(validated_questions, validated_answers)\n",
        "\n",
        "# Output the sizes of each set\n",
        "print(f\"Training set size: {len(train_questions)}\")\n",
        "print(f\"Validation set size: {len(val_questions)}\")\n",
        "print(f\"Test set size: {len(test_questions)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2175650-da42-461d-bffc-9e02c14a240d",
      "metadata": {
        "id": "d2175650-da42-461d-bffc-9e02c14a240d"
      },
      "source": [
        "# Tokenization and Data Preparation\n",
        "\n",
        "Use DialoGPT as the pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8550f7b4-4c26-47e1-98be-638ea67c5bda",
      "metadata": {
        "id": "8550f7b4-4c26-47e1-98be-638ea67c5bda"
      },
      "source": [
        "Tokenization: The tokenizer from the Hugging Face Transformers library is used to convert the conversations into tokenized input for the model.\n",
        "\n",
        "Managing Context: Need to ensure that previous conversation turns are included when generating a response.\n",
        "\n",
        "Padding and Truncation: We need to ensure the inputs are padded or truncated to a fixed length for batch processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23115de9-1047-46b3-943f-7a714145d862",
      "metadata": {
        "id": "23115de9-1047-46b3-943f-7a714145d862"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the pre-trained DialoGPT tokenizer\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence token\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "# Load the pre-trained DialoGPT model\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('microsoft/DialoGPT-small')\n",
        "\n",
        "\n",
        "# # Load DialoGPT tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "# # Load DialoGPT model\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# Load DialoGPT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load DialoGPT model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# # Function to tokenize conversations for multi-turn inputs\n",
        "# def tokenize_conversation(conversations):\n",
        "#     tokenized_data = []\n",
        "#     for conversation in conversations:\n",
        "#         # Tokenize and pad the conversation using DialoGPT tokenizer to a maximum length of 512\n",
        "#         encoded_input = tokenizer.encode(' '.join(conversation), return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "#         tokenized_data.append(encoded_input)\n",
        "#     return tokenized_data\n",
        "\n",
        "# # Tokenize the conversations\n",
        "# tokenized_conversations = tokenize_conversation(conversations)\n",
        "# print(f\"Tokenized {len(tokenized_conversations)} conversations.\")\n",
        "\n",
        "# Tokenize cleaned conversations\n",
        "def tokenize_conversation(questions, answers, max_length=512):\n",
        "    input_ids_list = []\n",
        "    attention_masks_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        # Concatenate question and answer for tokenization\n",
        "        encoded_input = tokenizer(question + \" \" + answer,\n",
        "                                  return_tensors='pt',\n",
        "                                  max_length=max_length,\n",
        "                                  padding='max_length',  # Ensures padding up to max_length\n",
        "                                  truncation=True)\n",
        "\n",
        "        input_ids = encoded_input['input_ids']  # Don't squeeze here, handle batch dimension later\n",
        "        attention_mask = encoded_input['attention_mask']\n",
        "\n",
        "        # Skip empty inputs (if any)\n",
        "        if input_ids.size(1) == 0:  # Check if input has a valid sequence length\n",
        "            print(f\"Empty input encountered for question-answer pair. Skipping.\")\n",
        "            continue  # Skip this pair if it's invalid\n",
        "\n",
        "        # Set labels as input_ids with padding token ignored (-100)\n",
        "        labels = input_ids.clone()\n",
        "        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding in labels\n",
        "\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_masks_list.append(attention_mask)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    # Stack input IDs, attention masks, and labels to create tensors\n",
        "    return torch.cat(input_ids_list, dim=0), torch.cat(attention_masks_list, dim=0), torch.cat(labels_list, dim=0)\n",
        "\n",
        "# Tokenization for the training data\n",
        "train_input_ids, train_attention_masks, train_labels = tokenize_conversation(train_questions, train_answers)\n",
        "val_input_ids, val_attention_masks, val_labels = tokenize_conversation(val_questions, val_answers)\n",
        "test_input_ids, test_attention_masks, test_labels = tokenize_conversation(test_questions, test_answers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d5bc44-306d-4027-8165-27d1cfaa2a0d",
      "metadata": {
        "id": "e9d5bc44-306d-4027-8165-27d1cfaa2a0d",
        "outputId": "b491af75-1f20-4c4b-e1d9-2dccdd81659b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Training 106192 input id's conversations.\n",
            "Tokenized Training 106192 attention masks conversations.\n",
            "\n",
            "Tokenized Validation 30340 input id's conversations.\n",
            "Tokenized Validation 30340 attention masks conversations.\n",
            "\n",
            "Tokenized Test 15171 input id's conversations.\n",
            "Tokenized Test 15171 attention masks conversations.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokenized Training {len(train_input_ids)} input id's conversations.\")\n",
        "print(f\"Tokenized Training {len(train_attention_masks)} attention masks conversations.\\n\")\n",
        "\n",
        "print(f\"Tokenized Validation {len(val_input_ids)} input id's conversations.\")\n",
        "print(f\"Tokenized Validation {len(val_attention_masks)} attention masks conversations.\\n\")\n",
        "\n",
        "print(f\"Tokenized Test {len(test_input_ids)} input id's conversations.\")\n",
        "print(f\"Tokenized Test {len(test_attention_masks)} attention masks conversations.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b22ba2e-cb03-4957-a1ad-e79857006166",
      "metadata": {
        "id": "0b22ba2e-cb03-4957-a1ad-e79857006166"
      },
      "source": [
        "# Create Data Loaders for the Train, Validation and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14757ee9-bef7-4608-bbd6-b29c5eb50ba8",
      "metadata": {
        "scrolled": true,
        "id": "14757ee9-bef7-4608-bbd6-b29c5eb50ba8",
        "outputId": "a9fb6f8d-62c0-4871-b222-224d39acf6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loader size: 106192\n",
            "Validation loader size: 30340\n",
            "Test loader size: 15171\n",
            "Number of steps in each epoch: 6637.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create DataLoader with padding\n",
        "def collate_fn(batch):\n",
        "    return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
        "\n",
        "def create_data_loader(tokenized_data, batch_size=4):\n",
        "    data_loader = DataLoader(tokenized_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    return data_loader\n",
        "\n",
        "# Create dataset and DataLoader for batching\n",
        "def create_dataloader(input_ids, attention_mask, labels, batch_size=16):\n",
        "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Running using CPU initially, so choosing batch size of 16\n",
        "# Smaller batch size can lead to noisier gradient updates but sometimes result in better generalization.\n",
        "# Larger batch sizes have more stable gradient updates but need more memory , learning rate needs tuning.\n",
        "batch_size = 16\n",
        "\n",
        "# # Create data loaders for the train, validation, and test sets\n",
        "# train_loader = create_data_loader(train_data)\n",
        "# val_loader = create_data_loader(val_data)\n",
        "# test_loader = create_data_loader(test_data)\n",
        "\n",
        "# print(f\"Training loader size: {len(train_loader.dataset)}\")\n",
        "# print(f\"Validation loader size: {len(val_loader.dataset)}\")\n",
        "# print(f\"Test loader size: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Create DataLoader for each dataset\n",
        "train_dataloader = create_dataloader(train_input_ids, train_attention_masks, train_labels, batch_size=batch_size)\n",
        "val_dataloader = create_dataloader(val_input_ids, val_attention_masks, val_labels, batch_size=batch_size)\n",
        "test_dataloader = create_dataloader(test_input_ids, test_attention_masks, test_labels, batch_size=batch_size)\n",
        "\n",
        "print(f\"Training loader size: {len(train_dataloader.dataset)}\")\n",
        "print(f\"Validation loader size: {len(val_dataloader.dataset)}\")\n",
        "print(f\"Test loader size: {len(test_dataloader.dataset)}\")\n",
        "\n",
        "print(f\"Number of steps in each epoch: {len(train_dataloader.dataset)/batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83464533-500b-4f46-ac8a-dee4d4d9fff6",
      "metadata": {
        "id": "83464533-500b-4f46-ac8a-dee4d4d9fff6"
      },
      "source": [
        "# Validate Data After Tokenization:\n",
        "\n",
        "Print out some tokenized examples to ensure that the tokenization process is working as expected, and there are no empty sequences or overly long sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370ff2d0-b1a2-4221-bc6d-de1407ee87c0",
      "metadata": {
        "id": "370ff2d0-b1a2-4221-bc6d-de1407ee87c0",
        "outputId": "785a5f35-a552-4179-a09d-9fe4aec5c6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0 Tokenized Input Lengths: [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]\n"
          ]
        }
      ],
      "source": [
        "for i, (input_ids, attention_mask, labels) in enumerate(train_dataloader):\n",
        "    print(f\"Batch {i} Tokenized Input Lengths: {[len(seq) for seq in input_ids]}\")\n",
        "    break  # Only print the first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06242379-8697-4b9f-9d14-694b8ffc3104",
      "metadata": {
        "id": "06242379-8697-4b9f-9d14-694b8ffc3104"
      },
      "source": [
        "# Training loop\n",
        "\n",
        "Using manual training loop for better control over the training process instead of using HuggingFace's Trainer and TrainingArguments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35fd8491-2d97-466d-bd70-d345a43ac2cc",
      "metadata": {
        "id": "35fd8491-2d97-466d-bd70-d345a43ac2cc"
      },
      "source": [
        "# Evaluation:\n",
        "\n",
        "Evaluation metrics like BLEU score or ROUGE Score to measure the quality of the generated conversations.\n",
        "\n",
        "BLEU Score: This metric is commonly used for evaluating machine translation and is calculated by comparing n-grams of the generated response against the reference responses.\n",
        "\n",
        "ROUGE Score: This metric is often used for evaluating text summarization and compares the overlap of n-grams between the generated response and reference texts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04734a67-c1d0-45a0-a0fb-71f394a20ce8",
      "metadata": {
        "id": "04734a67-c1d0-45a0-a0fb-71f394a20ce8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import AdamW  # Import AdamW from PyTorch to avoid warning when imported using transformers\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import time  # Import the time module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1532f4-4ac8-4f89-83c0-69fa9f866399",
      "metadata": {
        "id": "1a1532f4-4ac8-4f89-83c0-69fa9f866399"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "# Function to calculate BLEU score\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # Tokenize the sentences\n",
        "    reference_tokens = [ref.split() for ref in reference]  # Multiple reference sentences\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = sentence_bleu(reference_tokens, candidate_tokens)\n",
        "    return bleu_score\n",
        "\n",
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(reference, candidate):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(candidate, reference)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07bba7b-e529-4337-b59b-98d1a8d9cbdf",
      "metadata": {
        "id": "b07bba7b-e529-4337-b59b-98d1a8d9cbdf",
        "outputId": "9865e561-0a75-46ba-a83c-a1946cb4c3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.7598\n",
            "ROUGE Scores: [{'rouge-1': {'r': 0.8333333333333334, 'p': 0.8333333333333334, 'f': 0.8333333283333335}, 'rouge-2': {'r': 0.8, 'p': 0.8, 'f': 0.7999999950000002}, 'rouge-l': {'r': 0.8333333333333334, 'p': 0.8333333333333334, 'f': 0.8333333283333335}}]\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "reference_responses = [\"I am going to the store.\", \"I went to the store.\"]\n",
        "generated_response = \"I am going to the shop.\"\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = calculate_bleu(reference_responses, generated_response)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "# Calculate ROUGE score\n",
        "rouge_scores = calculate_rouge(reference_responses[0], generated_response)  # Just using the first reference\n",
        "print(f\"ROUGE Scores: {rouge_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48f60a6-c419-423e-9744-0ef94aee019b",
      "metadata": {
        "id": "a48f60a6-c419-423e-9744-0ef94aee019b"
      },
      "outputs": [],
      "source": [
        "# Define validation function with accuracy, precision, recall, and F1-score\n",
        "def validate_model(model, val_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Unpack the batch and move to CPU\n",
        "            input_ids = batch.squeeze().to(device)\n",
        "\n",
        "            # Perform a forward pass and calculate loss\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # Flatten input_ids and predictions for evaluation\n",
        "            all_preds.extend(predictions.flatten().cpu().numpy())\n",
        "            all_labels.extend(input_ids.flatten().cpu().numpy())\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Calculate BLEU and ROUGE scores\n",
        "    # For demonstration, you may want to keep some expected responses for evaluation\n",
        "    # Here you can use a sample of generated responses\n",
        "    # Note: In a real scenario, you would compare the generated response against actual reference responses\n",
        "    # reference_responses = [\"expected response 1\", \"expected response 2\"]\n",
        "    # generated_response = \"Your model's generated response here.\"  # Replace with actual output from your model\n",
        "\n",
        "    # bleu_score = calculate_bleu(reference_responses, generated_response)\n",
        "    # print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "    # rouge_scores = calculate_rouge(reference_responses[0], generated_response)  # Just using the first reference\n",
        "    # print(f\"ROUGE Scores: {rouge_scores}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f345a652-957a-4ea9-bb9a-0c76e8eebc7a",
      "metadata": {
        "id": "f345a652-957a-4ea9-bb9a-0c76e8eebc7a"
      },
      "outputs": [],
      "source": [
        "def check_for_nan(tensor, name=\"tensor\"):\n",
        "    if torch.isnan(tensor).any():\n",
        "        print(f\"Found NaN values in {name}.\")\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f72b9f-6087-4e70-8a32-2c1a5f1e17c8",
      "metadata": {
        "id": "08f72b9f-6087-4e70-8a32-2c1a5f1e17c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set device to CPU (since no GPU is available)\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move model to the CPU\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer (AdamW is a common choice for transformer models)\n",
        "# Disable weight decay\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5,weight_decay=0.0) # Reduced the learning rate from 5e-5 due to nan loss\n",
        "\n",
        "\n",
        "# Define training function\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, epochs=3):\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_train_loss = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}:\")\n",
        "\n",
        "        start_time = time.time()  # Start time for the epoch\n",
        "\n",
        "        # Training loop on  Train Data\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Unpack the batch and move to the CPU device\n",
        "            # input_ids = batch.squeeze().to(device)\n",
        "            input_ids, attention_mask, labels = batch # Assuming batch contains (input_ids, attention_mask, labels)\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Check for NaN values in input IDs\n",
        "            # if torch.isnan(input_ids).any():\n",
        "            if check_for_nan(input_ids, \"input_ids\") or check_for_nan(attention_mask, \"attention_mask\"):\n",
        "                print(\"Input IDs contain NaN values. Stopping training.\")\n",
        "                print(\"NaN detected in inputs, stopping training.\")\n",
        "                return\n",
        "\n",
        "            # Perform a forward pass and calculate loss\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss encountered at Step {step} in Epoch {epoch+1}\")\n",
        "                print(f\"Input IDs: {input_ids}\")\n",
        "                print(f\"Attention Mask: {attention_mask}\")\n",
        "                print(f\"Labels: {labels}\")\n",
        "                return  # Stop training to prevent further NaN propagation\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Check if gradients are NaN\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None and torch.isnan(param.grad).any():\n",
        "                    print(\"Encountered NaN gradients. Stopping training.\")\n",
        "                    return\n",
        "\n",
        "            # To prevent Gradient explosion due to large batch sizes, implement gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print every 10th step\n",
        "            if step % 10 == 0 and step > 0:\n",
        "                print(f\"Epoch {epoch+1},  Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "         # Calculate the time taken for the epoch\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Time taken for epoch {epoch + 1}: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        # Validation after each epoch using validation data\n",
        "        validate_model(model, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e51471-1ea3-47df-a10d-ae75f559e98c",
      "metadata": {
        "id": "95e51471-1ea3-47df-a10d-ae75f559e98c",
        "outputId": "aea60439-66d7-42b2-a650-714a12e53dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3:\n",
            "Epoch 1,  Step 10: Loss = 10.6100\n",
            "Epoch 1,  Step 20: Loss = 8.9878\n"
          ]
        }
      ],
      "source": [
        "# Train the model with 3 epochs\n",
        "train_model(model, train_dataloader, val_dataloader, optimizer, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85528a9-4860-4c97-b155-e7432bfb34c6",
      "metadata": {
        "id": "c85528a9-4860-4c97-b155-e7432bfb34c6"
      },
      "outputs": [],
      "source": [
        "# After training is complete, save the model and tokenizer\n",
        "output_dir = r'./fine_tuned_dialoGPT'  # Specify a directory where you want to save the model\n",
        "\n",
        "# Save the model and tokenizer after training is complete\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model and tokenizer saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98bcc648-9d96-41fd-80ef-a0a0a8aec969",
      "metadata": {
        "id": "98bcc648-9d96-41fd-80ef-a0a0a8aec969"
      },
      "source": [
        "# Inference:\n",
        "\n",
        "Build a chatbot interface where the model generates responses based on user inputs and conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5180809-81f8-4595-8e92-5ecc42750719",
      "metadata": {
        "id": "e5180809-81f8-4595-8e92-5ecc42750719"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the pre-trained DialoGPT model and tokenizer\n",
        "# model_name = \"microsoft/DialoGPT-small\"\n",
        "model_name = r'./fine_tuned_dialoGPT'  # or path to your trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate a response using the trained DialoGPT model\n",
        "def generate_response(message, history):\n",
        "    # Tokenize the input message and convert it to input_ids\n",
        "    new_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt').to(device)\n",
        "\n",
        "    # Concatenate the new input with the history of the conversation\n",
        "    bot_input_ids = new_input_ids\n",
        "    if history:\n",
        "        history_ids = tokenizer.encode(history, return_tensors='pt').to(device)\n",
        "        bot_input_ids = torch.cat([history_ids, new_input_ids], dim=-1)\n",
        "\n",
        "    # Generate a response\n",
        "    response_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Decode the generated response and add it to the history\n",
        "    response = tokenizer.decode(response_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    # Update the history\n",
        "    history = history + \" \" + message + \" \" + response\n",
        "\n",
        "    return response, history  # Return both response and updated history\n",
        "\n",
        "# Initialize Gradio ChatInterface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=generate_response,\n",
        "    examples=[{\"text\": \"Hello\", \"files\": []}],\n",
        "    title=\"ChatBot Powered by DialoGPT\",\n",
        "    description=\"A chatbot based on the DialoGPT model, fine-tuned for multi-turn conversations.\",\n",
        "    multimodal=False\n",
        ")\n",
        "\n",
        "# Launch the Gradio demo\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a summary report of this notebook\n",
        "\n",
        "def generate_summary_report(notebook_content):\n",
        "    \"\"\"Generates a summary report of a Jupyter Notebook content.\"\"\"\n",
        "\n",
        "    # Placeholder for a more sophisticated summarization method\n",
        "    # (e.g., using a pre-trained summarization model)\n",
        "    summary = f\"\"\"\n",
        "    This Jupyter Notebook appears to focus on fine-tuning a DialoGPT model for multi-turn conversations.\n",
        "    It includes sections for data loading, processing, model training, validation, and inference using Gradio.\n",
        "\n",
        "    Key aspects:\n",
        "    - Tokenization of input data using Transformers library.\n",
        "    - Data loading using PyTorch DataLoader.\n",
        "    - Model training using a custom training loop and AdamW optimizer.\n",
        "    - Validation using metrics such as accuracy, precision, recall, and F1-score.\n",
        "    - Evaluation of generated responses using BLEU and ROUGE scores.\n",
        "    - Chatbot interface using Gradio for user interaction.\n",
        "    \"\"\"\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Assuming notebook_content is the text content of your notebook\n",
        "# notebook_content = ...\n",
        "\n",
        "# Generate summary report\n",
        "# summary_report = generate_summary_report(notebook_content)\n",
        "\n",
        "# print(\"Summary Report:\\n\", summary_report)\n"
      ],
      "metadata": {
        "id": "xSOXmBMvyDTb"
      },
      "id": "xSOXmBMvyDTb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}