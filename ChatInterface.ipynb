{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "976f3c2a-1e96-45d5-9f85-588b60fbf9c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mthiruma\\AppData\\Local\\Temp\\ipykernel_23900\\3223938444.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict  = torch.load(model_path,map_location=torch.device(device))\n",
      "C:\\Users\\mthiruma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me: hi \n",
      "ChatFlix: how did you know?  i am the one with a job. but do not tell anybody! it is all about me, man... what are we doing here at home tonight anyway and how will they make them feel like this for us to come back on time again tomorrow night captain morgan?! he got nothing better than that right now.. wait till next year there was no other choice yet after midnight in your office room...now why does everything have any purpose of my own life these days ago last week evening or something crazy too soon before lunchtime today afternoon morning daynight never mind birthday nights later\n",
      "Me: what are birthday nights?\n",
      "ChatFlix: i am at a party. you know, my...  i do not like that one very much? it is the only way in! what about other days of your life for me to go home and watch television with him her back there on this day afternoon night time too late tomorrow morning dinner tonight evening lunch midnight mary peter killnight..! any advice out here yet or something around then again right now... hehe will be fine up by 1230 we shall have an alligator hunting accident soon enough usway down town later today baby jake nicklin wakeup 002037\n",
      "Me: how about evening lunch , lengthy inputs, peter killnight?  what is happening in downtown?\n",
      "ChatFlix: yes.  i am on the way to work... this will not be easy for me if you do it right now! how are we going?! can somebody help us out here please and see where all these people come from around town tonight with their new job tomorrow morning afternoon at noon start time of day night early dinner timeside working today midnight wake up already..... so ready yet again have a date there too late no thanks sir jackie barney just like that one did last week ago never mind him anyway\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# The tokenizer from Hugging Face's pre-trained DialoGPT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Load the model architecture (DialoGPT)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Fine Tuned models path\n",
    "model_path = r'./fine_tuned_dialoGPT/fine_tuned_dialoGPT_epoch3_step3600.pt'\n",
    "#model_path = r'C:\\Temp\\USD\\AAI-520\\Final Project\\Chatbot\\fine_tuned_dialoGPT\\fine_tuned_dialoGPT_epoch3_step3600.pt'\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# Load the fine-tuned model weights using torch.load (since it's a .pt file)\n",
    "model_state_dict  = torch.load(model_path,map_location=torch.device(device))\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(model_state_dict['model_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate a response using the trained DialoGPT model\n",
    "def generate_response(message, history):\n",
    "\n",
    "    print(\"Me: {}\".format(message))\n",
    "    # Tokenize the input and convert it to input_ids\n",
    "    new_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate a response from the model\n",
    "    # Increase the penalty to avoid repetition\n",
    "    #The higher the value, the more the model is penalized for repeating tokens.\n",
    "    #Top-k sampling limits the next token to be chosen from the top k most probable tokens.\n",
    "    #Top-p (nucleus) sampling chooses the next token from a dynamically sized group of tokens with cumulative probabilities adding up to p.\n",
    "    #The temperature parameter controls the randomness of predictions by scaling the logits before applying softmax.\n",
    "    response_ids = model.generate(new_input_ids, \n",
    "                                  max_length=128, \n",
    "                                  pad_token_id=tokenizer.eos_token_id, \n",
    "                                  repetition_penalty=1.2,   # Repetition penalty\n",
    "                                  top_k=50,                 # Top-k sampling\n",
    "                                  top_p=0.95,               # Nucleus sampling\n",
    "                                  do_sample=True,           # Enable sampling\n",
    "                                  no_repeat_ngram_size=3,   # Prevent repeating 3-word sequences\n",
    "                                  temperature=0.7           # Lower temperature for more focused response, Control randomness\n",
    "                                 )\n",
    "        \n",
    "    # Decode the generated response and clean up special tokens like <EOS>\n",
    "    response = tokenizer.decode(response_ids[:, new_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "    # Clean the response to remove any unexpected tokens or unwanted EOS markers. This limited the display within the chatbot \n",
    "    #to the EOS marker otherwise. \n",
    "    response = response.replace('<EOS>', '').strip()\n",
    "    \n",
    "    print(\"ChatFlix: {}\".format(response))\n",
    "\n",
    "    return response\n",
    "\n",
    "# Chat Interface in Gradio\n",
    "interface = gr.ChatInterface(\n",
    "    fn=generate_response,\n",
    "    title=\"ChatFlix using DialoGPT\",\n",
    "    description=\"A chatbot based on the DialoGPT model, fine-tuned for multi-turn conversations.\",\n",
    "    examples=[{\"text\": \"Hello\", \"files\": []}],\n",
    "    multimodal=False\n",
    ")\n",
    "\n",
    "interface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d7c4d-03a2-411f-b4fc-5e583b975fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
